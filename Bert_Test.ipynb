{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bert Test",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyMFfSBTksNp",
        "outputId": "4ded20a6-41cb-4806-f608-1dc52fc38737"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install tweet-preprocessor\n",
        "!pip install datasets\n",
        "!pip install bert_score"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (8.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: tweet-preprocessor in /usr/local/lib/python3.7/dist-packages (0.6.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.6.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.5.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (4.0.1)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.8)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: pyarrow>=1.0.0<4.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (20.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: bert_score in /usr/local/lib/python3.7/dist-packages (0.3.9)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from bert_score) (1.1.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from bert_score) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bert_score) (1.19.5)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from bert_score) (4.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from bert_score) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.7/dist-packages (from bert_score) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from bert_score) (1.8.1+cu101)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->bert_score) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->bert_score) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->bert_score) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->bert_score) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->bert_score) (2.4.7)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0->bert_score) (0.0.8)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0->bert_score) (0.10.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0->bert_score) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0->bert_score) (4.0.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0->bert_score) (0.0.45)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0->bert_score) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0->bert_score) (20.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->bert_score) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->bert_score) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->bert_score) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->bert_score) (2.10)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->bert_score) (3.7.4.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.0.1->bert_score) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers>=3.0.0->bert_score) (3.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=3.0.0->bert_score) (8.0.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=3.0.0->bert_score) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxUJyvBVrlMW"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "import pandas as pd\n",
        "import sys\n",
        "sys.path.insert(0, \"/content/drive/MyDrive/Colab Notebooks/\")"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuMWL8uG9AuA",
        "outputId": "f07770f9-1fdf-4ed8-b818-c4ba65fc6d82"
      },
      "source": [
        "from TextProcessor import *\n",
        "\n",
        "input = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/twitter_parsed_dataset.csv\")\n",
        "input_text = input['Text'].to_numpy()\n",
        "input_y = input['oh_label'].to_numpy()\n",
        "\n",
        "tp = TextProcessor(remove_punctuation=False,\n",
        "remove_stop_word=False,\n",
        "min_word_size=1,\n",
        "special_token_method=SpecialTokenMethod.REMOVE)\n",
        "\n",
        "parsed_tweets = tp.transform(input_text[0:100])\n",
        "print(parsed_tweets)\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['-pron- read -pron- in context . no change in meaning . the history of islamic slavery .', 'now -pron- idiot claim that people who try to stop -pron- from become a terrorist make -pron- a terrorist . islamically brain dead .', 'call -pron- sexist , but when -pron- go to an auto place , -pron- would rather talk to a guy', 'wrong , isis follow the example of mohammed and the quran exactly .', 'no no no no no no', \": saudi preacher who ' rape and torture ' -pron- five -year - old daughter to death be release after\", 'nooo not sexist but most woman be bad driver', \"go to make some pancake ..... don't hve any strawberry .... but -pron- hve banana ......\", ': how dare -pron- have feeling be a fantastic way to dehumanize someone .', ': there be something wrong when a girl win wayne rooney street striker', '-pron- be not on the autoblocker , and i do not run blockbot . block manually for now .', 'a good muslim be good despite -pron- bad religion , not because of -pron- .', '-pron- help everyone that ask . -pron- be not here to pass judgment on anyone that come look for help .', \"yeah , -pron- be call care about human life , idiot - something a genocidal daesh would'nt understand .\", 'otoh , eventually most of -pron- be like \" wait . harper ? -pron- be randi harper ? \" and -pron- still do not know how to adjust to this .', 'roflmao . the putin delusion factory be run full speed and the putin pay troll be distribute .', ': if kat & amp ; andre be drown in a river & amp ; -pron- only have the chance to save one of -pron- ... what type of sandwich would -pron-', ': literally on this site today', 'juxtaposition .', 'woo can not wait to see what happen ! ! !', 'i tend to document -pron- hair color experiment for later . & gt;.&gt ; that pigtail pic be a few year old .', 'accord to holly from geordie shore , \" sassy - halfway between slut and classy \"', 'vomit .', 'aww -pron- be look forward to ash and camilla butt head with the promo girl . the drama would have be great', 'food poisoning .', '-pron- be a muslim . -pron- be brain dead . -pron- repeat what other have say a million time .', ': what do -pron- think of the two team face sudden death on monday ?', 'be -pron- time for channel to reconsider -pron- association with the dangerous man ?', 'male follower , -pron- be time to collect -pron- trash .', ': -pron- be not sexist -pron- just do not believe in some of the idiosyncrasy of certain female out there . male too .', ': guy who sell drug get more girl than guy with degree , but that say more about woman than man', '-pron- be sure this tweet will be use in court one day . when -pron- yell rape the light turn off', 'yay . i go last year and have to make new friend . i hate make new friend . i do not know anyone else go .', 'preeetty easy to spot which house be producer rental', 'a lady be currently drive this bus ... funnily enough -pron- be hit the curb twice', 'i get that -pron- be probably go to defend -pron- friend regardless , but what -pron- do be shitty , and then -pron- follow -pron- w / more shit .', '-pron- do not think kat & amp ; andre deserve to be in the competition anymore', 'ash find -pron- inner bogun ewwww', \"tonight 's -pron- kitchen rules elimination be complete bullshit .\", 'be gamergate finally eat -pron- own tail ?', 'tbh , i feel bad anytime anyone actively enter this fight without know about the personal cost .', '-pron- be not sexist , but girl should not wrestle', 'peace be important . but freedom be even more important .', 'oh another trip around the country ? will -pron- be cook shark ? because this show just jump one and maim -pron- beyond recognition', 'if -pron- want to understand the lie of muslims live in peace with jews , read ibn warraq', 'yes .', 'why do -pron- like lamb and leftover in disneyland ?', ': isis execute a christian priest paul jacob in mosul , be hold in captivity for month . : isis execute ...', 'for real ? -pron- be not sexist , but man be superior .', 'kat , a massive c*nt . the big ever on', 'this fucking potato be blow my mind . duck fat . -pron- guy . figuratively dying of bliss .', 'kat and andre fuck up -pron- be both oxygen thief', 'actually , -pron- be not ! -pron- mistake . -pron- be beyond all comprehension . girl be just rubbish at geography', ': first grader ask about the photo of mike brown . such innocence', ': -pron- be not sexist or anything but ... (no really -pron- be not , hear -pron- out ) if -pron- be a female comedienne -pron- have a lot to do to ma', \"d'awww . -pron- feel like wonder woman .\", 'unlike -pron- , who be hilariously ignorant and can not spell \" racist \" -pron- be sorry but girl be just rly unfunny', 'kat be one nasty bitch', '-pron- be a total liar , just like -pron- pedophile prophet . un soldier do not burn people alive .', 'show -pron- some picture of -pron- behead people and reinstate slavery . leftist moral equivalent be stupid .', 'since when do give a fuck about science ? go sell -pron- advertiser cosmetic wuth -pron- wild claim', 'be not that -pron- goal ?', ': kat and andre fuck up -pron- be both oxygen thief', 'aww yay ash and camila', '-pron- favorite part of this trip have to be all the amazing new sock .', 'both daesh and shia militia be drive by a religion of hatred and bigotry and there will be no freedom in tikrit , regardless .', 'booo kat and andre do not deserve to get through', 'just -pron- guess ... -pron- be now in -pron- tweet as proof if -pron- come true . -pron- know -pron- would rig the draft so the avs could get seth jones .', 'ok ... who should stay ? and who should go ?', 'yes ... and real shit ...', ': what i learn from', 'if the muslim world ever produce anything but tyrant , dictator , fascist , and fanatic -pron- would support -pron- .', 'cc', ': the world -pron- live in', 'islam do not answer anything . -pron- pretend to answer with illogical and delusional superstition .', 'why do team look shocked when -pron- announce the final score will be reveal at a different location ? have not -pron- watch before ?', 'how would poor lynn and tony feel lose to girl who microwave couscous , use tin tomato , packet capsicum and packet prosciutto', \": huckabee : us give iran and cuba ' ice cream and candy '\", 'lynne and tony be go home - -pron- be too likeable', '-pron- should be attack everyone that follow a religious cult of hated and murder like islam .', 'here be a whole bunch of -pron- little fucker .', '\" girl sport \" give -pron- away here -pron- do not even understand why girl sport be televise', 'pssst . . - this be not ethical journalism . -pron- never happen . ed be not a reliable source . & lt;3', ': because -pron- make sure -pron- be not alone , and have multiple witness , before fire a male colleague .', '-pron- be the liar . -pron- behead jewish man in one day and sell -pron- woman and child into slavery .', \": -pron- investigate gamergate 's late conspiracy theory , and , as expect , -pron- be full of half - truth and nonsense :\", 'can -pron- follow for a dm ?', ': egypt threaten military strike on yemen houthis ( iran - back ) if -pron- threaten strategic bab al - manab shipping lane htt', 'yessssss .', '-pron- be not sexist , but feminist make -pron- sick in how -pron- go about fighting for equality . sorry .', 'the difficulty be find time to find those ally while figure out what to do with the one -pron- have get .', ': in the film , jennifer lopez play a rapist that do not know what the iliad be . and -pron- be the * protagonist * !', 'islam be not a race , microbrain , -pron- be a death cult .', 'but amazon be generally a terrible place to buy any clothing other than sock . & gt;.&gt ;', 'the stupidist argument be the one rationalize the barbarity of islam .', 'rofl . jews use to live on % of arabian peninsula . who be commit genocide against who ?', 'this may be -pron- favorite tweet by rogue , ever . because -pron- assume -pron- twitter handle be -pron- unix login .', ': wow : : by balance stone , an artist find peace', ': good morning piss off american patriot ! cop in danger ?', ': rt : katie and nikki need to be deport from australia rt:.vid .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VikTrv4XzF5J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac4a6503-9471-49e7-e23e-59912f654276"
      },
      "source": [
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "trainset = [[x, input_y[i]]\n",
        "                 for i, x in enumerate(parsed_tweets)]\n",
        "random.shuffle(trainset)\n",
        "\n",
        "X = pd.DataFrame(trainset).iloc[:, 0].to_numpy()\n",
        "y = pd.DataFrame(trainset).iloc[:, 1].to_numpy()\n",
        "\n",
        "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "print(X_train_raw)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[': egypt threaten military strike on yemen houthis ( iran - back ) if -pron- threaten strategic bab al - manab shipping lane htt'\n",
            " 'male follower , -pron- be time to collect -pron- trash .'\n",
            " 'this fucking potato be blow my mind . duck fat . -pron- guy . figuratively dying of bliss .'\n",
            " '-pron- should be attack everyone that follow a religious cult of hated and murder like islam .'\n",
            " 'woo can not wait to see what happen ! ! !'\n",
            " 'if -pron- want to understand the lie of muslims live in peace with jews , read ibn warraq'\n",
            " 'for real ? -pron- be not sexist , but man be superior .'\n",
            " 'a lady be currently drive this bus ... funnily enough -pron- be hit the curb twice'\n",
            " 'islam do not answer anything . -pron- pretend to answer with illogical and delusional superstition .'\n",
            " ': what i learn from'\n",
            " 'i get that -pron- be probably go to defend -pron- friend regardless , but what -pron- do be shitty , and then -pron- follow -pron- w / more shit .'\n",
            " ': what do -pron- think of the two team face sudden death on monday ?'\n",
            " 'ash find -pron- inner bogun ewwww'\n",
            " ': -pron- be not sexist -pron- just do not believe in some of the idiosyncrasy of certain female out there . male too .'\n",
            " 'kat , a massive c*nt . the big ever on'\n",
            " 'nooo not sexist but most woman be bad driver'\n",
            " 'unlike -pron- , who be hilariously ignorant and can not spell \" racist \" -pron- be sorry but girl be just rly unfunny'\n",
            " 'here be a whole bunch of -pron- little fucker .'\n",
            " ': there be something wrong when a girl win wayne rooney street striker'\n",
            " 'islam be not a race , microbrain , -pron- be a death cult .'\n",
            " \": -pron- investigate gamergate 's late conspiracy theory , and , as expect , -pron- be full of half - truth and nonsense :\"\n",
            " 'roflmao . the putin delusion factory be run full speed and the putin pay troll be distribute .'\n",
            " '-pron- be the liar . -pron- behead jewish man in one day and sell -pron- woman and child into slavery .'\n",
            " 'now -pron- idiot claim that people who try to stop -pron- from become a terrorist make -pron- a terrorist . islamically brain dead .'\n",
            " 'pssst . . - this be not ethical journalism . -pron- never happen . ed be not a reliable source . & lt;3'\n",
            " ': good morning piss off american patriot ! cop in danger ?'\n",
            " 'yes ... and real shit ...'\n",
            " '-pron- be a total liar , just like -pron- pedophile prophet . un soldier do not burn people alive .'\n",
            " '\" girl sport \" give -pron- away here -pron- do not even understand why girl sport be televise'\n",
            " ': if kat & amp ; andre be drown in a river & amp ; -pron- only have the chance to save one of -pron- ... what type of sandwich would -pron-'\n",
            " 'tbh , i feel bad anytime anyone actively enter this fight without know about the personal cost .'\n",
            " ': rt : katie and nikki need to be deport from australia rt:.vid .'\n",
            " 'oh another trip around the country ? will -pron- be cook shark ? because this show just jump one and maim -pron- beyond recognition'\n",
            " 'be not that -pron- goal ?'\n",
            " 'the difficulty be find time to find those ally while figure out what to do with the one -pron- have get .'\n",
            " \": saudi preacher who ' rape and torture ' -pron- five -year - old daughter to death be release after\"\n",
            " 'wrong , isis follow the example of mohammed and the quran exactly .'\n",
            " \": huckabee : us give iran and cuba ' ice cream and candy '\"\n",
            " 'aww -pron- be look forward to ash and camilla butt head with the promo girl . the drama would have be great'\n",
            " 'vomit .'\n",
            " \"go to make some pancake ..... don't hve any strawberry .... but -pron- hve banana ......\"\n",
            " 'otoh , eventually most of -pron- be like \" wait . harper ? -pron- be randi harper ? \" and -pron- still do not know how to adjust to this .'\n",
            " 'ok ... who should stay ? and who should go ?'\n",
            " '-pron- be not sexist , but feminist make -pron- sick in how -pron- go about fighting for equality . sorry .'\n",
            " 'show -pron- some picture of -pron- behead people and reinstate slavery . leftist moral equivalent be stupid .'\n",
            " '-pron- be sure this tweet will be use in court one day . when -pron- yell rape the light turn off'\n",
            " ': kat and andre fuck up -pron- be both oxygen thief'\n",
            " 'just -pron- guess ... -pron- be now in -pron- tweet as proof if -pron- come true . -pron- know -pron- would rig the draft so the avs could get seth jones .'\n",
            " ': wow : : by balance stone , an artist find peace'\n",
            " 'actually , -pron- be not ! -pron- mistake . -pron- be beyond all comprehension . girl be just rubbish at geography'\n",
            " 'preeetty easy to spot which house be producer rental'\n",
            " '-pron- be not sexist , but girl should not wrestle'\n",
            " 'call -pron- sexist , but when -pron- go to an auto place , -pron- would rather talk to a guy'\n",
            " 'accord to holly from geordie shore , \" sassy - halfway between slut and classy \"'\n",
            " ': in the film , jennifer lopez play a rapist that do not know what the iliad be . and -pron- be the * protagonist * !'\n",
            " 'this may be -pron- favorite tweet by rogue , ever . because -pron- assume -pron- twitter handle be -pron- unix login .'\n",
            " 'why do team look shocked when -pron- announce the final score will be reveal at a different location ? have not -pron- watch before ?'\n",
            " \"yeah , -pron- be call care about human life , idiot - something a genocidal daesh would'nt understand .\"\n",
            " ': first grader ask about the photo of mike brown . such innocence'\n",
            " ': -pron- be not sexist or anything but ... (no really -pron- be not , hear -pron- out ) if -pron- be a female comedienne -pron- have a lot to do to ma'\n",
            " 'be -pron- time for channel to reconsider -pron- association with the dangerous man ?'\n",
            " ': guy who sell drug get more girl than guy with degree , but that say more about woman than man'\n",
            " 'food poisoning .'\n",
            " 'rofl . jews use to live on % of arabian peninsula . who be commit genocide against who ?'\n",
            " 'cc' ': literally on this site today'\n",
            " 'why do -pron- like lamb and leftover in disneyland ?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0Bayspa_MfL"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased', use_fast=True)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVB0Ju2ok4JW"
      },
      "source": [
        "import torch\n",
        "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from datasets import load_metric\n",
        "import numpy as np\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.encodings = [self.tokenize_tweet(tweet) for tweet in encodings]\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.encodings[idx]\n",
        "        item = {key: torch.tensor(val) for key, val in text.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def tokenize_tweet(self, tweet_text):\n",
        "        return self.tokenizer(tweet_text, truncation=True, padding=True)\n",
        "    \n",
        "class BertDataModule():\n",
        "    def __init__(self,x_tr,y_tr,x_test,y_test,tokenizer, batch_size=16):\n",
        "        super().__init__()\n",
        "        self.tr_text = x_tr\n",
        "        self.tr_label = y_tr\n",
        "        self.test_text = x_test\n",
        "        self.test_label = y_test\n",
        "        self.tokenizer = tokenizer\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.setup()\n",
        "\n",
        "    def setup(self):\n",
        "        self.train_dataset = Dataset(encodings=self.tr_text,  labels=self.tr_label,tokenizer=self.tokenizer)\n",
        "        #self.val_dataset= Dataset(encodings=self.val_text, labels=self.val_label,tokenizer=self.tokenizer)\n",
        "        self.test_dataset =Dataset(encodings=self.test_text, labels=self.test_label,tokenizer=self.tokenizer)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_dataset,batch_size= self.batch_size, shuffle = True , num_workers=4)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader (self.val_dataset,batch_size= 16)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader (self.test_dataset,batch_size= 16)\n",
        "\n",
        "class SBMBertClassifier:\n",
        "    def __init__(self, n_epochs=3, lr=2e-5, batch_size=16):\n",
        "        config = AutoConfig.from_pretrained('bert-base-cased', num_labels=1)\n",
        "        self.model = AutoModelForSequenceClassification.from_config(config)\n",
        "        self.bertscore = load_metric(\"bertscore\")\n",
        "        self.lr = lr\n",
        "        self.n_epochs = n_epochs\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def compute_metrics(self, eval_pred):\n",
        "        acc_metric = load_metric(\"accuracy\")\n",
        "        logits, labels = eval_pred\n",
        "        predictions = np.argmax(logits, axis=-1)\n",
        "        #self.bertscore.add_batch(predictions=predictions, references=labels)\n",
        "        return acc_metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "    def train(self, datamodule):\n",
        "        args = TrainingArguments(\n",
        "            \"sbm\",\n",
        "            evaluation_strategy = \"epoch\",\n",
        "            learning_rate=self.lr,\n",
        "            per_device_train_batch_size=self.batch_size,\n",
        "            per_device_eval_batch_size=self.batch_size,\n",
        "            num_train_epochs=self.n_epochs,\n",
        "            weight_decay=0.01,\n",
        "            load_best_model_at_end=True,\n",
        "        )\n",
        "        trainer = Trainer(\n",
        "            self.model,\n",
        "            args,\n",
        "            train_dataset=datamodule.train_dataset,\n",
        "            eval_dataset=datamodule.test_dataset,\n",
        "            tokenizer=datamodule.tokenizer,\n",
        "            compute_metrics=self.compute_metrics)\n",
        "        trainer.train()\n",
        "\n",
        "    def score():\n",
        "        metric = load_metric(\"bertscore\")\n",
        "        for batch in dataset:\n",
        "            inputs, references = batch\n",
        "            predictions = model(inputs)\n",
        "            metric.add_batch(predictions=predictions, references=references)\n",
        "        score = metric.compute()\n",
        "        print(score)\n",
        "\n",
        "    def validate(self):\n",
        "        self.trainer.test()\n",
        "\n",
        "    def test(self):\n",
        "        self.trainer.test()\n",
        "\n",
        "    def predict(self, text):\n",
        "        # TODO ??? No predict in pl.trainer\n",
        "        return self.model(text)\n",
        "\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1Mao_c_BWUj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "4277f3c2-bc2d-4afc-c38d-597ee089a301"
      },
      "source": [
        "datamodule = BertDataModule(X_train_raw, y_train, X_test_raw, y_test, tokenizer)\n",
        "cls = SBMBertClassifier()\n",
        "cls.train(datamodule)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [15/15 04:36, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.150196</td>\n",
              "      <td>0.818182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.540801</td>\n",
              "      <td>0.818182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.383013</td>\n",
              "      <td>0.818182</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([3, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([3, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([3, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eb-mP7TjkuWE"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5k-x_uXC6rF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfUPKUk3IyCs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rD1WfQXzF5Jl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOf74wX9HTWz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJVK2ID1Vq-B"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}